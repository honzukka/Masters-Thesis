\chapter{Background}
\label{chapter:background} 


Before we present our contributions, we provide an overview of the work other researchers that we build on as well as an explanation of concepts that we use in later parts of the thesis. At the core of this chapter are the answers to the following questions:

\begin{itemize}
    \item How can we predict what an image will look like when projected onto a scene?
    \item Given a texture image, how can we generate a different image that represents the same texture?
\end{itemize}

The former question is part of the field of projection mapping, while the latter represents the field of texture synthesis.

% Projection mapping and texture synthesis are both well-established research areas and in this section, we explain the theories and methods used in later parts of this thesis.

% In the case of projection mapping, we largely avoid discussing the current state of the art and focus on general theory of projection mapping instead. This is because all existing methods, such as those of \citet{Siegl2017} and \citet{Willi2017}, are addressing the issue of how to calibrate projectors and compute compensations faster while pushing the quality of projection mapping to the limits defined by projection hardware (discussed in section \ref{section:intro-problem_setting}). We want to show that statistics-based projection mapping can be fundamentally better than pixel-based projection mapping. Therefore we perform projection mapping fully in software where we set maximum projector brightness ourselves and computations can be much longer than real time. This allows us to solve any pixel-based projection mapping problem completely and then compare our method against this ideal pixel-based reference.

% We do, however, provide an overview of the state of the art in texture synthesis because this problem is far from solved even in theory. The reason for that is that each method has to rely on a rigorous definition of what makes two textures look alike. In our experiments, we implement some of the best current synthesis methods and evaluate their performance in the context of projection mapping.

\section{Projection Mapping}
\label{section:background-projection_mapping}

In order to predict projection appearance, we first need to understand how projectors work, get an intuition of what to roughly expect when projecting on various surfaces and then understand the theory behind light transport. Finally, we can combine these concepts and study projector-camera systems and how they can help us answer our question.

\subsection{Projectors}
\label{section:background-projection_mapping-projectors}

Projectors are devices that transfer images onto backgrounds, such as screens, walls and other physical objects. The more traditional kind of projectors are film projectors, that shine a bright light through a film and a set of lenses. Nowadays, digital projectors are more common, but the core principle is the same.

{\color{red} TODO: maybe a figure of a projector?}

As briefly mentioned in section \ref{section:intro-problem_setting}, there are limitations to the way projectors are constructed that have a large impact on how projection mapping is done. To get a better intuition of these limitations, we shall now explain how one type of digital projectors works. We will focus the so-called Digital Light Processing (DLP) projector which is widely used both in cinemas as well as home setups. Other common types of digital projectors are for example LCD or laser projectors.

\subsubsection{DLP Projectors}
\label{section:background-projection_mapping-projectors-DLP}

{\color{red} TODO: somehow cite the source here?}
% https://en.wikipedia.org/wiki/Digital_Light_Processing

As mentioned in the previous paragraph, DLP projectors or principally the same as film projectors -- they have a bright lamp and a set of lenses that direct the light towards physical objects such as screens. Instead of film, however, they need a more complicated device that the light shines through to project an image.

DLP projectors project images by filtering the bright white light of the lamp. First, the light goes through a Digital Micromirror Device (DMD). This device is divided into many tiny mirrors which roughly correspond to individual pixels of the projected image. Each of these mirrors can either reflect light directly into the lens, or into a heat sink which absorbs it and does not let it through. This allows the projector to turn pixels off and on. Grayscales (pixel intensities between full and zero) are produced by rapidly toggling the mirror between the lens and the heat sink. If, for example, a pixel is on 50\% of the time and off 50\% of the time, the resulting intensity is exactly between full and zero.

Next is color. After light passes though the DMD, it hits a rapidly spinning color wheel which is split into a number of sectors: red, green, blue and sometimes also transparent. At any point, all light from the DMD passes through a single color filter, sending a single-channel image towards the lens. By sending out many single-channel images in rapid succession, however, the projector creates the illusion of sending a three-channel image. Coordination with the DMD is therefore needed to send each channel at a specific intensity.

{\color{red} TODO: multi-chip DLPs?}

{\color{red} TODO: figure of how DLP projectors work}

\subsubsection{Hardware Limitations}
\label{section:background-projection_mapping-projectors-limitations}

Based on the way DLPs work, it is easy to see that the process of projecting an image is fairly inefficient and limiting, especially when it comes to the maximum brightness and minimum brightness (also known as black level) of a projector.

Maximum brightness is simply limited by the brightness of the lamp. The brighter the lamp, the more energy it consumes and the more energy is wasted when dark colors or blacks are being projected.

Minimum brightness is related to the ability of the projector to absorb light which is not reflected directly towards the lens. The more light is absorbed inside the projector, the more the projector heats up. This is why DLP projectors with deep blacks need to be large enough to cool themselves down efficiently. If a DLP projector is not able to absorb all the light, some of it is let through towards the screen and is visible as dim gray instead of black.

Different projector technologies have different limitations. For example laser projectors are generally more efficient and brighter because they produce exactly the light color which is needed, as opposed to filtering out white light. But even laser projectors cannot be infinitely bright and even a projector which can produce true black color will not be able to project it onto scenes under external illumination.

An important thing to keep in mind when projecting onto screens, but especially crucial for projection mapping, is that it is impossible to reproduce arbitrary appearance. The less of a particular color a given background reflects, the more difficult it is to project that color onto it.

\subsection{Intuition on Projection Appearance}
\label{section:background-projection_mapping-projection_intuition}

To study projection apperance rigorously, we need light transport theory which we present in the following section. First, however, we believe it is useful to provide the reader with an intuition on what to expect when we install a projector and press Play.

The appearance of an object is given by the light it reflects. Light is the visible portion of electromagnetic radiation and consists of photons at various wavelengths. Wavelengths which human vision is sensitive to are approximately between 380 and 780 nm (\citet{PBRT3e}). Shorter wavelengths appear blue, middle ones are green and longer ones are red. Reflectance of an object is defined by the so-called \textit{spectral power distribution} (SPD) which describes what proportion of incoming light is reflected at each wavelength.

{\color{red} TODO: figure of reflectance of lemon peel from PBRT}

Projectors are then nothing but light sources. As a matter of fact, each of the millions of tiny mirrors inside the DMD of a DLP projector can be thought of as a separate light source. Projector light has an SPD of its own, describing how much power it carries at each wavelength. When it interacts with a surface, something roughly corresponding to SPD multiplication takes place. If an objects does not reflect any light at 450 nm, all of it will be absorbed and will not reach our eyes.

See figure for examples of how projection interacts with various backgrounds.

{\color{red} TODO: figure of projection interacting with various backgrounds}

We will now study this process more carefully using light transport theory.

% Projected images are not the same as observed images. Before reaching our eyes, projector light is reflected from at least one surface. This reflection is important because based on the properties of the surface, it can modify projector light in almost an arbitrary way. How can we then predict what will observers see when they look at our projection as we have set out to find out at the beginning of this section?

% {\color{red} TODO: figure of the same projection on various surfaces (if Mulholland Drive was projected on an orange screen, the blue box would appear grey and nobody would understand the movie! \(\rightarrow \) mention in paragraph below?)}

% \subsubsection{Achieving desired appearance}
% \label{section:background-projection_mapping-projections-screen}

% It might not be immediately obvious, but this is a question not only us, projection mappers, but also movie directors and teachers are interested in. Colors in movies and lecture slides often have crucial meaning and they need to be reproduced faithfully, so that the meaning is conveyed. The solution in cinemas and classrooms is to use a screen and control ambient light (it is usually recommended to project in dark rooms). But why does it work?

% As a matter of fact, even in this case, some amount of projection mapping needs to be done. Even in a cinema, the projected image is not the same as the observed image. Therefore, it needs to be modified as it is projected, so that the reproduction is faithful. However, the following aspects of a controlled environment with a screen make these necessary modifications trivial:

%  \begin{enumerate}
%      \item The fact that a cinema hall always looks the same allows for the same modification to be used every time and not re-calibration is necessary
%      \item A screen has a uniform surface of constant color. This means that each pixel of a projection is distorted in exactly the same way and thus only global modifications are needed
%      \item A screen is designed to make the distortion as small as possible. The modifications are therefore not only global, but also very simple and can be found quickly by manual tweaking
%  \end{enumerate}

% What happens in a general case when projecting onto arbitrary objects? In this thesis, we assume our environment to be static because we are not interested in the speed with which we can perform calibration. Point 1) is therefore valid even in our "general" case. Points 2) and 3), however, do not hold anymore. There is nothing we can say about the compensation function which needs to be applied to each image before it is projected. It is different for each pixel and can be be arbitrarily complicated.

% Estimating this compensation function is at the core of projection mapping as we have defined it (that is, making objects look like the projection, rather than complementing them by the projection). If we know how a given object distorts our projection, we can obtain the compensation function by inverting this process. In the next section, we will cover the theory behind light transport which studies precisely these phenomena and which projection mapping methods, including ours, heavily build upon.

\subsection{Light Transport Theory}
\label{section:background-projection_mapping-light_transport}

Light transport theory is the study of how things look. This depends on a number of variables, such as object geometry and materials, light sources and a point of view. The most common application of this theory can be found in rendering, for example of architectural visualizations or video games. There, a scene is given along with a camera view and the goal is to compute what is visible and how it looks. Our case is very similar. We have a scene with a projector (i.e. a light source) and our goal is to predict what the scene will look like from a point of view that focuses on the projection. Once we know that, we are only a step away from controlling the projection in a way that the scene looks exactly the way we want -- from projection mapping.

In this section, we provide a brief overview of light transport theory. For a more comprehensive coverage, we refer the reader to our source, Physically Based Rendering: From Theory to Implementation by \citet{PBRT3e}.

\subsubsection{Radiance}
\label{section:background-projection_mapping-light_transport-radiance}

As mentioned in \ref{section:background-projection_mapping-projection_intuition}, all we see is light. Measuring light is therefore the crux of understanding how things look. The fundamental quantity we are interested in is \textit{radiance}. It is defined as the amount of photons \(Q\) per unit projected area \(A^\perp \) per unit solid angle \(\omega\) per unit of time \(t\):

\begin{equation}
    \label{eq:radiance}
    L = \frac{dQ}{dA^\perp d\omega dt}
\end{equation}

{\color{red} TODO: figure of radiance}

{\color{red} TODO: mention that radiance is wavelength-dependent and how this is handled in our equations?}

{\color{red} TODO: mention luminance?}

We will now present the relationship between objects, light sources and the radiance incoming onto our camera sensor or eye retina. This relationship is called the \textit{rendering equation}.

\subsubsection{Rendering Equation}
\label{section:background-projection_mapping-light_transport-rendering_equation}

The rendering equation describes the amount of outgoing radiance from point \(x\) in a scene towards a direction \(\mathbf{v}\):

\begin{equation}
    \label{eq:rendering_equation}
    L(x \rightarrow \mathbf{v}) = \int_{\Omega(x)} L(x \leftarrow \mathbf{u}) f_r(x, \mathbf{u} \rightarrow \mathbf{v}) \cos \theta d\mathbf{u} + E(x \rightarrow \mathbf{v})
\end{equation}

{\color{red} TODO: figure of rendering equation}

where \(L(x \leftarrow \mathbf{u})\) is the radiance arriving at \(x\) from direction \(\mathbf{u}\), \(\Omega(x)\) is the hemisphere oriented to the direction of the surface normal at \(x\), \(f_r\) is the spatially-varying bidirectional reflectance distribution function (SVBRDF) which determines surface reflectance for each point \(x\), incoming direction \(\mathbf{u}\) and outgoing direction \(\mathbf{v}\), and \(\theta\) is the angle between \(u\) and the surface normal at \(x\). Finally, \(E(x \rightarrow \mathbf{v})\) is the radiance emitted from \(x\) towards \(\mathbf{v}\) in case \(x\) lies on a light source.

The main idea of the equation is that radiance leaving towards \(\mathbf{v}\) from \(x\) is the sum of reflected and emitted radiance. It assumes empty space between objects, therefore no light scattering occurs between them. This means that radiance is constant along straight lines and the equation is recursive -- \(L(x \leftarrow \mathbf{u}) = L(y \rightarrow -\mathbf{u})\) where \(y\) is the point seen from \(x\) in the direction of \(\mathbf{u}\).

Apart from assuming that our objects are in vacuum, the equation also assumes that light is reflected from the same point it arrived at. This is generally not the case as can be seen for example with human skin where light under the surface before it is reflected. Both subsurface scattering and participating media are handled in modern renderers which generalize the rendering equation to account for these phenomena ({\color{red} TODO: reference papers that do that?}). For our purposes, however, it is sufficient to be familiar with the basic form of the rendering equation.

\subsubsection{Towards Projection Mapping}
\label{section:background-projection_mapping-light_transport-towards_projection_mapping}

What is the difference between rendering and projection mapping and what makes projection mapping so challenging? In rendering, Monte Carlo integration is often use to solve the rendering equation and thus determine the amount of radiance reflected towards the camera. Scene geometry, materials and the positions and properties of light sources are known ({\color{red} TODO: figure?}). In projection mapping, we do know the amount of radiance that is reflected towards the camera -- it is our desired appearance of the scene. However, everything else is unknown. From scene geometry and materials to the source of the radiance -- the projection image ({\color{red} TODO: figure?}).

Projection mapping algorithms therefore do not typically solve the full light transport of the whole scene, but instead they build on top of assumptions, such as ({\color{red} TODO: 1-2 assumptions}), and provide estimates. We shall now review some of the common approaches to projection mapping found in state-of-the-art methods.

\subsection{Projector-Camera Systems}
\label{section:background-projection_mapping-procams}

The key idea in projection mapping is to use a camera to observe the projection and provide information on how to adapt it to achieve desired appearance. This system as a whole is called the \textit{projector-camera system}, commonly shortened as \textit{procam}.

As mentioned in section \ref{section:background-projection_mapping-light_transport-towards_projection_mapping}, general projection mapping, or solving full light transport in a scene where the only known quantity is radiance arriving at the camera sensor, is very challenging and simplifying assumptions are therefore needed to make the problem tractable. One such assumption is that each camera pixel is influenced by exactly one projector pixel, or at most by a small neighborhood around that pixel. Although this is not the case for example when light bounces around the scene multiple times, methods that use this assumption are reasonably general.

Methods that assume 1:1 pixel correnpodencies between camera and projector are usually split into two parts: \textit{geometric calibration} and \textit{radiometric calibration}. The goal of geometric calibration is to determine those correspondencies. Once we know which projector pixels affect which camera pixels, and therefore the pixels of our desired appearance, we need to find our \textit{how} they affect them. This is done using radiometric calibration which determines the relationship between the intensities of each pair of corresponding projector and camera pixels.

{\color{red} TODO: doesn't this implicitly require per-pixel matching? investigate and explain the consequences this would have for our method!}

Methods that try to account for global illumination effects typically use different assumptions to make the problem tractable, for example that a scene can only be composed of Lambertian objects of constant color ({\color{red} TODO: cite example (Siegl, 17?)}). Those that do attempt to solve general light transport take several hours to calibrate provide only an approximate solution which can only account for some global illumination effects ({\color{red} TODO: cite example (Wetzstein, 07)}).

We will now briefly cover a few methods that rely on geometric and radiometric calibration and one method that attempts to solve general light transport. For a complete overview of the state of the art in projection mapping, see \citet{Grundhofer2018}. We have chosen the methods we are about to describe for the following reasons:

\begin{itemize}
    \item To show that even the latest methods working in simplified scenarios without global illumination effects are severly limited by projector hardware
    \item To show how general light transport can be solved with procams what the challenges when taking this approach are ({\color{red} TODO: tie it together with the LT matrix stuff you're doing later on})
\end{itemize}

\subsubsection{Geometric Calibration}
\label{section:background-projection_mapping-procams-geometric_calibration}

The goal of geometric calibration is to find a point \(M_c = [u_c, v_c]^T\) in the camera image for each point \(M_p = [u_p, v_p]^T\) in the projector image such that the value of the latter determines the value of the former. These correspondencies are usually established via a third point, \(P = [x, y, z]\) which is located in the scene.

The relationship between \(M_c\) and \(P\) is determined by the \textit{intrinsic} and \textit{extrinsic} matrices:

\begin{equation}
    \label{eq:camera_equation}
    \begin{bmatrix}
        M_c \\
        1
    \end{bmatrix} =
    \begin{bmatrix}
        f_x & c & u \\
        0 & f_y & v \\
        0 & 0 & 1 
    \end{bmatrix} \cdot
    \begin{bmatrix}
        \mathbf{R} & \mathbf{t}
    \end{bmatrix} \cdot
    \begin{bmatrix}
        P \\
        1
    \end{bmatrix}
\end{equation}

{\color{red} TODO: fix the matrix, so that the dimensions check out}

where the intrinsic matrix is formed by camera focal lengths \(f_x\) and \(f_y\) (there are two focal lengths because they are expressed in camera pixels which are generally rectangular), skewness \(c\) of image axes, and principal point coordinates \([u, v]^T\) (the intersection of optical axis and image plane which is generally not in the center of the image). The extrinsic matrix is then formed by rotation \(\mathbf{R}\) and translation \(t\) which convert between world coordinates and camera coordinates.

The most commonly used method for finding the parameters of \ref{eq:camera_equation} was introduced by \citet{Zhang1999}. They are estimated by taking at least two photos of a planar pattern at various orientations.

We will now outline two recent methods to perform geometric calibration. The first one requires user assistance, while the second one is fully automatic.

\textbf{One camera, one projector and a calibration board.} The first method, introduced by \citet{Yang2016}, uses a calibration board containing a random dot pattern. First, the camera is calibrated using the approach of \citet{Zhang1999}. Then the projector is calibrated by projecting another dot pattern onto the board using the inverse camera method. The whole process needs around 10 views of the calibration board according to experiments in the paper.

\textbf{Two cameras, one projector.} An entirely automatic self-calibration method was presented by \citet{Willi2017}. Their method first establishes pixel correspondencies between the two cameras and then continues to estimate the intrinsic and extrinsic matrix, as well as the 3D point cloud of the scene. Finally, the projector is calibrated using this information. It is worth noting that this method works also for any larger number of projectors and cameras in which case camera pairs are sorted by the quality of their pixel correspondencies. Calibration is first done for the best pair and other devices are incorporated iteratively, improving the overall estimate.

Both methods work only on static scenes and take several minutes to complete. Real time methods also exist, but those usually rely on some prior knowledge about scene geometry ({\color{red} TODO: find example}) or on time-consuming pre-calibration followed by incremental updates ({\color{red} TODO: find example}).

{\color{red} TODO: assumptions on diffuse surfaces that make the correspondencies 1:1?}

\subsubsection{Radiometric Calibration}
\label{section:background-projection_mapping-procams-radiometric_calibration}

Once correspondencies between projector and camera pixels have been established using one of the methods described in section \ref{section:background-projection_mapping-procams-geometric_calibration}, radiometric calibration can be performed. In general, the goal is to find a color-mapping function \(f\) such that

\begin{equation}
    \label{eq:radiometric_calibration}
    c_c = f(c_p)
\end{equation}

where \(c_c\) is the color of a camera pixel and \(c_p\) is the color of the corresponding projector pixel. We describe one method that assumes \(f\) to be linear and one method that allows for arbitrary \(f\).

\textbf{Linear color-mapping function.} One of the first projection mapping methods was presented by \citet{Grossberg2004}. There, the relationship between \(c_c\) and \(c_p\) is specified as follows:

\begin{equation}
    \label{eq:linear_color_mapping}
    c_c = p_c(V p_p(c_p) + F)
\end{equation}

where \(p_p\) is a non-linear projector response function which turns input pixel values into projector brightness, \(p_c\) is a non-linear camera response that converts radiance arriving at the camera sensor to an output pixel value, \(F = [F_R, F_G, F_B]\) is the environment light term which is independent of the projector, and finally \(V\) is a \(3 \times 3\) color mixing matrix which captures the relationship between projector and camera channels and their interactions with spectral reflectance.

{\color{red} TODO: tidy up the notation}

Both \(p_p\) and \(p_c\) are independent of the scene and can be estimated separately. \(V\) and \(F\) are per-pixel and scene-dependent and can be estimated by projecting and capturing 6 calibration images.

The disadvantage of this method are that the linear color mixing matrix is not an accurate model for example when using DLP projectors described in section \ref{section:background-projection_mapping-projectors-DLP}. DLP projectors sometimes form an image by composing red, green, blue and white channels together. The extra white channel may result in non-linear behavior. Another disadvantage is that if the color gamut of the projector struggles to reproduce color required by the compensation model, it results in clipping artifacts and lowered contrast {\color{red} TODO: verify this precisely}.

\textbf{Non-linear color-mapping function with a global optimization step.} To solve these problems, \citet{Grundhofer2015} presented an improved method which allows for an arbitrary color-mixing function. This function is estimated by obtaining up to \(6^3\) samples and then applying thin-plate spline interpolation on these samples. This sampling process takes several minutes but once it is completed, compensation can be performed in real time.

On top of that, the paper also introduces an important idea ({\color{red} TODO: it has been used before, though\dots}) of content-based projection image compensation. This is a separate step which can also take several minutes and consists of optimizing per-pixel (or per-patch) coefficients which scale the final luminance values of the image. The goal of this optimization is to minimize clipping errors cause by limited color gamut of the projector and overall increase image contrast.

As a result, this method achieves very good results in scenes which have minimal inter-reflection that cannot be modeled when 1:1 correspondence between projector and camera pixels is assumed. The idea of the global optimization step is also very important for our thesis because it suggests that algorithms that try to optimize the L2 distance between camera image and desired appearance do not always yield the best-looking results.

\subsubsection{Inverse Light Transport}
\label{section:background-projection_mapping-procams-inverse_lt}

\section{Texture Synthesis}
\label{section:background-texture_synthesis}

{\color{red} TODO}