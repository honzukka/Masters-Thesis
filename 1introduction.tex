\chapter{Introduction}
\label{chapter:intro}

Most people nowadays are familiar with \textit{projection}, the act of transferring images from film or computer memory onto screens in cinemas and classrooms. But while those screens are made specifically for being projected onto, this is not the case for other objects, for example building fa√ßades. The geometry and material properties of these objects deform projected images and change their appearance. This is why projecting them is not enough -- they first need to be edited (\textit{mapped}) in a way that their final appearance is what we expect. Projecting onto arbitrary objects in such a way is called \textit{projection mapping} and it is the main topic of this thesis.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/01-sydney_opera_house.jpg}
        \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/01-sydney_opera_house_projection.jpg}
        \caption{Source: \citet{ImageProjectionMappingExampleSydney}}
    \end{subfigure}
    \caption{The Sydney Opera House without (a) and with (b) projection mapping.}
    \label{fig:intro_example_sydney}
\end{figure}

One of the first uses of projection mapping (also sometimes called \textit{video mapping} or \textit{spatial augmented reality}) was in the character of Madame Leota in The Haunted Mansion Disneyland ride, which opened in 1969. There, a video of a human face was projected onto a static head (see fig.~\ref{fig:intro_example_leota}).

\begin{figure}
    \centering
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/01-Leota.jpg}
        \caption*{Final appearance \(r(\bm{p})\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \begin{subfigure}{\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/01-Leota-no_head.jpg}
            \caption*{Background}
        \end{subfigure}
        
        \begin{subfigure}{\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/01-Leota-only_head.jpg}
            \caption*{Projection image \(\bm{p}\)}
        \end{subfigure}
    \end{subfigure}
    \caption{The character of Madame Leota in Disneyland's Haunted Mansion ride is one of the earliest examples of projection mapping. A video of her face is projected onto a neutrally coloured static form inside a glass ball to give an illusion that the form speaks (\citet{MadameLeota}). Included is our simplified decomposition of the final appearance \(r(\bm{p})\) into background and projection image \(\bm{p}\).}
    \label{fig:intro_example_leota}
\end{figure}

Nowadays, projection mapping has become widespread. It is used to augment reality by artists from all over the world in galleries, museums and outdoor spaces. One prominent example is projection mapping on buildings (see fig.~\ref{fig:intro_example_sydney}) which is done in cities during festivals and on other special occasions.

Let us now briefly describe what projection mapping entails and which of its problems we aim to address in this thesis.

\section{Problem Setting}
\label{section:intro-problem_setting}

Simply put, projection mapping is projecting onto surfaces that are not primarily intended for projection and often have complex texture or geometry. In this thesis, we formulate the projection mapping problem as follows (see fig.~\ref{fig:intro_grossberg} for example images):

\begin{enumerate}
    \item We begin with a scene (see fig.~\ref{fig:intro_grossberg-background})
    \item We choose what we would want it to look like after projection (i.e.~the \textit{desired appearance} \(\bm{y}\)) (see fig.~\ref{fig:intro_grossberg-desired_appearance})
    \item The task is to create a \textit{compensated projection image} \(\bm{p}\) that will look like our desired appearance \(\bm{y}\) when projected onto our scene (see fig.~\ref{fig:intro_grossberg-compensation})
    \item We project the compensated image (see fig~\ref{fig:intro_grossberg-new_appearance})
\end{enumerate}

There is a wide body of research focusing on creating such compensated images \(\bm{p}\) automatically given a scene and a desired appearance \(\bm{y}\). In one of the earliest papers of the field, \citet{Grossberg2004} project a series of special calibration images onto a scene and capture their appearance by a camera. Using these \textit{camera images} \(r(\bm{p})\), they are able to estimate how each pixel of the projection influences the appearance of that particular scene. Once this calibration is ready, they are able to quickly compute compensation images \(\bm{p}\) on the fly and only project those (see fig.~\ref{fig:intro_grossberg} for an example). The combination of a projector and a camera is common in projection mapping and such systems are called \textit{projector-camera systems} (see fig.~\ref{fig:intro_procam} for an illustration).

\begin{figure}
    \centering    
    \begin{subfigure}{0.9\textwidth}
        \centering
        \begin{subfigure}{0.34\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/01-grossberg-desired_appearance.jpg}
            \caption{Desired appearance \(\bm{y}\)}
            \label{fig:intro_grossberg-desired_appearance}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.24\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/01-grossberg-background.jpg}
            \caption{Background}
            \label{fig:intro_grossberg-background}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.34\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/01-grossberg-uncompensated.jpg}
            \caption{Uncompensated appearance \(r(\bm{y})\)}
            \label{fig:intro_grossberg-uncompensated}
        \end{subfigure}
        
        \begin{subfigure}{0.34\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/01-grossberg-compensation_image.jpg}
            \caption{Compensated projection image \(\bm{p}\)}
            \label{fig:intro_grossberg-compensation}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.34\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/01-grossberg-new_appearance.jpg}
            \caption{Final appearance \(r(\bm{p})\)}
            \label{fig:intro_grossberg-new_appearance}
        \end{subfigure}
    \end{subfigure}
    \caption{Automatic projection mapping from \citet{Grossberg2004}}
    \label{fig:intro_grossberg}
\end{figure}

\begin{figure}
    \centering
    \def\svgwidth{0.8\textwidth}
    \input{images/figures/01-procam.pdf_tex}
    \caption{An illustration of a projector-camera system. An (uncompensated) image \(\bm{p}\) is projected onto a scene. Its appearance \(r(\bm{p})\) is captured by the camera. The desired appearance \(\bm{y}\) is shown in the top-left corner.}
    \label{fig:intro_procam}
\end{figure}

Since then, there have been many advances in the field, the summary of which can be found in a state-of-the-art report by \citet{Grundhofer2018}. The latest projector-camera systems are able to calibrate themselves automatically after they are placed in a scene, recalibrate when scene illumination changes or when objects in the scene are transformed, both rigidly (i.e.~without deformation) and non-rigidly. Some can do this in real time. Because of the sheer complexity of general projection mapping, however, no single method can do all these things at once. For example, methods that handle non-rigid transformations will often rely on object tracking which requires the object to have markers on it. Such methods might also break when illumination changes significantly.

There is, however, one fundamental characteristic that all current projection mapping methods share. When computing the compensated projection image \(\bm{p}\), they match the camera image \(r(\bm{p})\) with the desired appearance \(\bm{y}\) pixel by pixel. Explicitly, their goal is the following:

\begin{equation}
    \label{eq:projection_mapping-per_pixel}
    \min_{\bm{p}} \sum_{i=1}^{m \cdot n} || r(\bm{p})_{(i)} - \bm{y}_{(i)} ||
\end{equation}

where \((i)\) selects the \(i\)-th pixel of an \(m \times n\) image. Note that function \(r\) that represents the process of projecting \(\bm{p}\) and capturing its appearance by a camera and it is determined by the projector, the camera and the scene.

This approach is limited by projector hardware. Every projector has finite brightness which means that pixels of the camera image \(r(\bm{p})\) cannot be made arbitrarily bright. In scenes with external illumination, it is also impossible to make pixels of \(r(\bm{p})\) arbitrarily dark since projectors can only add light and not subtract it. In the notation introduced above this means that for any projector image \(\bm{p}\), we have

\begin{equation}
    \label{eq:projection_mapping-limitations}
    \forall i:\; \bm{a}_{(i)} \leq r(\bm{p})_{(i)} \leq \bm{b}_{(i)}
\end{equation}

where \(\bm{a}\) and \(\bm{b}\) are constant vectors that determine brightness values for each pixel and that depend on the projector and the scene.

For every projector and scene, we can therefore find a desired appearance \(\bm{y}\) whose pixels are too dark or too bright such that they cannot be mapped using the recipe in eq.~\ref{eq:projection_mapping-per_pixel}:

\begin{equation}
    \label{eq:projection_mapping-hard_image}
    \exists i:\; \bm{y}_{(i)} < \bm{a}_{(i)} \lor \bm{y}_{(i)} > \bm{b}_{(i)}
\end{equation}

See fig.~\ref{fig:intro_pixels_vs_stats} for examples.

\begin{figure}[ht]
    \centering    
    \begin{subfigure}{\textwidth}
        \centering
        \begin{subfigure}{0.2\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/01-pixels_vs_stats-pixels_target.jpg}
            \caption*{Desired appearance \(\bm{y_1}\)}
            \vspace*{5mm}
            \label{fig:intro_pixels_vs_stats-pixels_target}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.2\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/01-pixels_vs_stats-bg.jpg}
            \caption*{Scene}
            \vspace*{10mm}
            \label{fig:intro_pixels_vs_stats-pixels_bg}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.2\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/01-pixels_vs_stats-pixels_opt.jpg}
            \caption*{Compensated projection image \(\bm{p_1}\)}
            \label{fig:intro_pixels_vs_stats-pixels_opt}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.2\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/01-pixels_vs_stats-pixels_proj.jpg}
            \caption*{Final appearance \(r(\bm{p_1})\)}
            \label{fig:intro_pixels_vs_stats-pixels_proj}
        \end{subfigure}
        
        \begin{subfigure}{0.2\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/01-pixels_vs_stats-stats_target.jpg}
            \caption*{Desired appearance \(\bm{y_2}\)}
            \vspace*{5mm}
            \label{fig:intro_pixels_vs_stats-stats_target}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.2\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/01-pixels_vs_stats-bg.jpg}
            \caption*{Scene}
            \vspace*{10mm}
            \label{fig:intro_pixels_vs_stats-stats_bg}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.2\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/01-pixels_vs_stats-stats_opt.jpg}
            \caption*{Compensated projection image \(\bm{p_2}\)}
            \label{fig:intro_pixels_vs_stats-stats_opt}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.2\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/01-pixels_vs_stats-stats_proj.jpg}
            \caption*{Final appearance \(r(\bm{p_2})\)}
            \label{fig:intro_pixels_vs_stats-stats_proj}
        \end{subfigure}
    \end{subfigure}
    \caption{This illustration shows two examples of projection mapping. In the top row, image \(\bm{y_1}\) cannot be mapped onto the given scene because of the dark areas that reflect little light. Even when projector brightness is set to maximum in those areas (white pixels in image \(\bm{p_1}\)) according to eq.~\ref{eq:projection_mapping-per_pixel}, the final appearance \(r(\bm{p_1})\) does not match the desired appearance \(\bm{y_1}\). In other terms, \(\bm{y_1}\) fulfils eq.~\ref{eq:projection_mapping-hard_image}. On the other hand, image \(\bm{y_2}\) can be mapped without issues because its bright areas are well aligned with highly reflective parts of the scene and vice versa. Texture source: \citet{Gatys2015}, modified}
    \label{fig:intro_pixels_vs_stats}
\end{figure}

In this thesis, we reformulate the projection mapping recipe from eq.~\ref{eq:projection_mapping-per_pixel} to overcome this limitation. We then present a technique that uses our new recipe to achieve results that have not been achieved before when projecting a particular class of images -- textures.

\section{Key Idea}
\label{section:intro-key_idea}

In this section we outline the main idea of overcoming hardware limitations of projectors as introduced above and explain why we focus on projecting textures. For clarity, we provide only a high-level explanation and leave more detailed formal treatment of textures and other concepts for Section~\ref{section:background-texture_synthesis}.

Textures (e.g.~a pebble beach) have the interesting property that when their features (e.g.~individual pebbles) are shuffled, the texture still looks the same. We say that a texture has multiple \textit{realizations}. Figure~\ref{fig:intro_pixels_vs_stats} shows an example of two different realizations (\(\bm{y_1}\) and \(\bm{y_2}\)) of the same texture. Moreover, in that particular example, one can be mapped onto a given scene, while the other cannot. This is the key idea of our thesis and here is how we apply it:

\begin{itemize}
    \item We assume a texture \(\bm{y}\) whose given realization is difficult to map onto a given scene because of projector limitations (\(\bm{y}\) fulfils eq.~\ref{eq:projection_mapping-hard_image})
    \item Out of all possible realizations \(\bm{y_i}\) of that texture we find \(\bm{y^\prime}\) that is the easiest to map (\(||r(\bm{p^\prime}) - \bm{y^\prime}|| \leq ||r(\bm{p}) - \bm{y_i}||\) for all \(\bm{p}\), \(\bm{p^\prime}\) and \(i\))
    \item We find the appropriate compensated projection image \(\bm{p^\prime}\) that minimizes \(||r(\bm{p^\prime}) - \bm{y^\prime}||\)
\end{itemize}

How can we find both \(\bm{y^\prime}\) and \(\bm{p^\prime}\) efficiently? It turns out that \(\bm{y^\prime}\) can be found implicitly by a simple modification of the recipe from eq.~\ref{eq:projection_mapping-per_pixel}. Instead of trying to match individual pixels of camera image \(r(\bm{p})\) and desired appearance \(\bm{y}\), we try to force \(r(\bm{p})\) and \(\bm{y}\) to be realizations of the same texture by matching their summary statistics \(f\):

\begin{equation}
    \label{eq:projection_mapping-statistics}
    \min_{\bm{p}} || f(r(\bm{p})) - f(\bm{y}) ||
\end{equation}

where \(f\) represents a set of summary statistics of an image and has the property that \(f(\bm{y_1}) \sim f(\bm{y_2})\) \textit{iff} images \(\bm{y_1}\) and \(\bm{y_2}\) are realizations of the same texture. Note that eq.~\ref{eq:projection_mapping-statistics} is strictly more general than eq.~\ref{eq:projection_mapping-per_pixel} because \(r(\bm{p}) = \bm{y}\) implies \(f(r(\bm{p})) = f(\bm{y})\), but the reverse implication does not hold.

Figure~\ref{fig:intro_pixels_vs_stats} serves as an example of how our recipe could be applied. With given desired appearance \(\bm{y_1}\), scene (as shown in the figure) and projector, \(\bm{p_2}\) is a solution to eq.~\ref{eq:projection_mapping-statistics}. \(f(r(\bm{p_2})) \sim f(\bm{y_1})\), but \(r(\bm{p_2}) \neq \bm{y_1}\). On the other hand, if the desired appearance was \(\bm{y_2}\), then \(\bm{p_2}\) would solve both eq.~\ref{eq:projection_mapping-per_pixel} and eq.~\ref{eq:projection_mapping-statistics}.

A suitable set of statistics \(f\), an algorithm that can optimize according to eq.~\ref{eq:projection_mapping-statistics} and the ability to integrate both into a projector-camera system are the three main components of our method for projection mapping of textures which we present in this thesis.

\section{Contributions}
\label{section:intro-contributions}

This thesis focuses on a problem of projection mapping exemplified in the upper row of figure~\ref{fig:intro_pixels_vs_stats} where a projector is not able to match the desired appearance of a texture due to hardware limitations. Our contributions to solving this problem are the following:

\begin{itemize}
    \item We reformulate the recipe of projection mapping as described in eq.~\ref{eq:projection_mapping-per_pixel} to accommodate it better to the projection mapping of textures. Our proposed recipe is described in Section~\ref{section:intro-key_idea}
    \item Our recipe needs a suitable set of summary statistics (referred to as \(f\) in eq.~\ref{eq:projection_mapping-statistics}) which would capture the essence of a texture and be the same across all realizations of the same texture. This problem has been well studied in the field of \textit{texture synthesis} (state-of-the-art report by \citet{Raad2018}) which we introduce and integrate a modified version of neural-based summary statistics by \citet{Gatys2015} into our method
    \item We simulate a projector-camera system in software and implement our method on top of the simulation. It consists of an optimization algorithm that solves eq.~\ref{eq:projection_mapping-statistics} in PyTorch
    \item We guide the reader through three experiments, whose aim is to evaluate the three key components of our proposed method. Here are the components and a brief summary of the experiments:
    \begin{enumerate}
        \item \textbf{Algorithm to solve eq.~\ref{eq:projection_mapping-statistics}.} In a toy example, our method is able to generate a texture that is both a realization of a given texture and adapted to fit a given scene well
        \item \textbf{Texture statistics \(f\).} We evaluate a few known texture models in the context of our method and conclude that in certain challenging conditions our method outperforms current pixel-based projection mapping methods as expected. Example shown in fig.~\ref{fig:intro_result_teaser}
        \item \textbf{Projector-camera system integration.} Most projection mapping examples in this section project onto flat textured walls. However, our method also works well in virtual 3D scenes with complex geometry and global illumination effects. This constitutes the first step towards real-world application
    \end{enumerate}
\end{itemize}

\begin{figure}[ht]
    \centering    
    \begin{subfigure}{\textwidth}
        \centering
        \begin{subfigure}{0.26\textwidth}
            \centering
            \vspace*{10mm}
            \includegraphics[width=\textwidth]{images/01-results_teaser-target.jpg}
            \caption{Desired appearance \(\bm{y}\)}
            \label{fig:intro_results_teaser_target}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.72\textwidth}
            \centering
            \caption*{Compensated projection image \(\bm{p}\)} 
            \begin{subfigure}{0.49\textwidth}
                \centering
                \begin{tikzpicture}
                    \draw (0, 0) node[inner sep=0] {\includegraphics[width=\textwidth]{images/01-results_teaser-stats_opt.jpg}};
                    \fill[black] (0.95, -1.75) rectangle ++(1.4, 0.5);
                    \node [anchor=center] at (1.65, -1.48) {{\color{white} Ours}};
                \end{tikzpicture}
                \caption{}
                \label{fig:intro_results_teaser_stats_opt}
            \end{subfigure}
            \hfill
            \begin{subfigure}{0.49\textwidth}
                \centering
                \begin{tikzpicture}
                    \draw (0, 0) node[inner sep=0] {\includegraphics[width=\textwidth]{images/01-results_teaser-pixels_opt.jpg}};
                    \fill[black] (0.15, -1.75) rectangle ++(2.2, 0.5);
                    \node [anchor=center] at (1.25, -1.48) {{\color{white} Baseline}};
                \end{tikzpicture}
                \caption{}
                \label{fig:intro_results_teaser_pixels_opt}
            \end{subfigure}
        \end{subfigure}
        
        
        \begin{subfigure}{0.26\textwidth}
            \centering
            \vspace*{10mm}
            \begin{tikzpicture}
                \draw (0, 0) node[inner sep=0] {\includegraphics[width=\textwidth]{images/01-results_teaser-bg.jpg}};
                \draw[line width=0.6mm, white] (-0.6, 0.4) ellipse (0.3cm and 0.45cm);
                \draw[line width=0.6mm, magenta] (0.2, 0.85) ellipse (0.55cm and 0.3cm);
            \end{tikzpicture}
            \caption{Scene}
            \vspace*{5mm}
            \label{fig:intro_results_teaser_bg}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.72\textwidth}
            \centering
            \caption*{Final appearance \(r(\bm{p})\)}
            \begin{subfigure}{0.49\textwidth}
                \centering
                \begin{tikzpicture}
                    \draw (0, 0) node[inner sep=0] {\includegraphics[width=\textwidth]{images/01-results_teaser-stats_proj.jpg}};
                    \fill[black] (0.95, -1.75) rectangle ++(1.4, 0.5);
                    \node [anchor=center] at (1.65, -1.48) {{\color{white} Ours}};
                    \draw[line width=0.8mm, white] (-0.85, 0.5) ellipse (0.45cm and 0.65cm);
                    \draw[line width=0.8mm, magenta] (0.25, 1.15) ellipse (0.75cm and 0.5cm);
                \end{tikzpicture}
                \caption{}
                \label{fig:intro_results_teaser_stats_proj}
            \end{subfigure}
            \hfill
            \begin{subfigure}{0.49\textwidth}
                \centering
                \begin{tikzpicture}
                    \draw (0, 0) node[inner sep=0] {\includegraphics[width=\textwidth]{images/01-results_teaser-pixels_proj.jpg}};
                    \fill[black] (0.15, -1.75) rectangle ++(2.2, 0.5);
                    \node [anchor=center] at (1.25, -1.48) {{\color{white} Baseline}};
                    \draw[line width=0.8mm, white] (-0.85, 0.5) ellipse (0.45cm and 0.65cm);
                    \draw[line width=0.8mm, magenta] (0.25, 1.15) ellipse (0.75cm and 0.5cm);
                \end{tikzpicture}
                \caption{}
                \label{fig:intro_results_teaser_pixels_proj}
            \end{subfigure}
        \end{subfigure}
    \end{subfigure}
    \caption{Result from one of our experiments. Highlighted in pink is an area where our scene reflects little light. Highlighted in white is a saturated area which mostly reflects orange hues. Image~\ref{fig:intro_results_teaser_pixels_proj} shows that this presents a challenge for the baseline method which performs conventional per-pixel projection mapping. Colours of the corresponding areas in the desired appearance \(\bm{y}\) mostly get absorbed and colours from the scene are visible even after compensation. On the other hand, image~\ref{fig:intro_results_teaser_stats_proj} shows that our method deals with this situation better by rearranging the desired appearance \(\bm{y}\) and thus achieving better overall compensation. Texture source: \citet{Pixar128}, modified}
    \label{fig:intro_result_teaser}
\end{figure}

\section{Thesis Structure}
\label{section:intro-thesis_structure}

This thesis is organized as follows. In chapter~\ref{chapter:background}, we provide an overview of the state of the art in projection mapping and texture synthesis and explain in detail the methods which we build on. In chapter~\ref{chapter:methods}, we describe our method and its implementation. In chapter~\ref{chapter:results}, we present the experiments and analyse their results. Lastly, in chapter~\ref{chapter:conclusions} we conclude the thesis and discuss future work, particularly with respect to application in real-world projector-camera systems.
