\chapter{Methods}
\label{chapter:methods}

We now have all the building blocks needed to construct our method that implements projection mapping of textures by forcing the camera image to be a realization of the same texture as the desired appearance, instead of matching them pixel by pixel which is the usual approach (see section \ref{section:intro-key_idea} for more details on our key idea).

We will first present an overview of our projection mapping pipeline and then focus on its main components.

\section{Pipeline Overview}
\label{section:methods-pipeline_overview}

\begin{figure}[]
    \centering
    \def\svgwidth{\textwidth}
    \input{images/figures/03-pipeline.pdf_tex}
    \caption{An overview of our pipeline with \(y\) as input and \(p\) that minimizes eq. \ref{eq:projection_mapping-statistics} as output. The blue part of the diagram is essentially a texture synthesis pipeline by \citet{Gatys2015}. The green part (the rendering function) is our extension that enables projection mapping. Instead of computing a set of statistics \(f\) on the synthesized image \(p\) directly as \citet{Gatys2015} does (blue arrow), we first project \(p\) to obtain \(x(p)\) and only then compute \(f\). The red projector symbol shows how \(p\) is projected onto a glass ball in our scene and the white scissors symbol shows how the camera image is cropped because we only want to match a portion of the scene. The whole process is an optimization loop with \(p\) initialized to white noise and progressively refined until \(x(p)\) is a realization of the same texture as \(y\) is. Texture source: \citet{Pixar128}}
    \label{fig:methods_pipeline}
\end{figure}

Projection mapping is typically done with a projector-camera system (see fig. \ref{fig:intro_procam} for an example). However, we have decided to implement our system entirely as a software pipeline with simulated projector and camera for the following reasons:

\begin{itemize}
    \item A software implementation serves as necessary reference for future work. If something does not work in the controlled environment of a simulator, it will not work in the real world either
    \item Hardware systems come with challenges of its own, like signal-to-noise ratio, dynamic environment and time constraints. We want to focus on solving one challenge at a time, starting with projection quality
    \item Software pipelines are easier to modify and allow for rapid prototyping and experimentation
\end{itemize}

The goal of our pipeline is to minimize the expression in eq. \ref{eq:projection_mapping-statistics}. The main input is an image \(y\) which represents the desired appearance for our scene. Other inputs are the scene and projector which determine a rendering function \(x\) which creates a camera image \(x(p)\) out of a projector image \(p\). A projector image \(p\) which minimizes eq. \ref{eq:projection_mapping-statistics} is the output of the pipeline. See fig. \ref{fig:methods_pipeline} an illustration of how the pipeline works.

We build on top of a pipeline for texture synthesis used in \citet{Gatys2015} and extend it by the rendering function \(x\). The original synthesis pipeline is differentiable and relies on the gradient-based L-BFGS optimizer to synthesize a texture. Therefore, it is key that our \(x\) differentiable as well. This allows us to rely on L-BFGS to minimize eq. \ref{eq:projection_mapping-statistics}.

In section \ref{section:background-texture_synthesis}, we have reviewed several texture synthesis methods that could potentially be used in our method. We have decided to use \citet{Gatys2015} over a patch-based method such as \citet{Efros2001} for the following reasons:

\begin{itemize}
    \item It achieves good results for a large variety of textures
    \item It defines a texture model which can be (and has been, as we will discuss later) improved to achieve better results while keeping the rest of the pipeline fixed
    \item It is very easy to adapt for projection mapping and does not impose any restrictions on the rendering function. In fact, a gradient-free version of the pipeline could theoretically be run on a physical projector-camera system
    \item Using an optimizer to gradually improve the result allows us to obtain a reasonable result quickly and improve it if more time is available
\end{itemize}

We will now describe the two main components of our pipeline -- the rendering function \(x\) and the texture model \(f\) -- in more detail.

\section{Rendering Function}
\label{section:methods-rendering_function}

The rendering function in our pipeline has the following form:

\begin{equation}
    \label{eq:rendering_function}
    x(p, \phi) = y  
\end{equation}

where \(p\) is the projector image, \(\phi\) are scene parameters, such as geometry, materials, projector and camera position and orientation and so on. \(y\) is the camera image whose statistics are matched against those of the input texture image (i.e. desired appearance). \(x\) needs to be differentiable because the statistics matching is done via gradient-based optimization.

\(x\) represents the act of projecting an image onto a scene and then capturing that projection. This is a fairly complex task, especially if the scene has arbitrary geometry, materials and external illumination. Luckily, in certain special scenarios, \(x\) can be very simple to formulate and fast to compute. We take advantage of this and use two different versions of \(x\) in our pipeline, a simple one to verify basic functionality and obtain reference results, and a general one to support arbitrary scenes.

\subsection{Simple Rendering Function}
\label{section:methods-rendering_function-simple}

Here is a scenario in which the light transport between the projector and the camera is easy to model:

\begin{itemize}
    \item The projector is pointed directly at a flat wall, with a right angle between its optical axis and the surface of the wall
    \item The surface of the wall is completely diffuse, meaning it looks matte and its appearance is not view-dependent
    \item The camera coincides with the projector
    \item We ignore light that is reflected from the wall into the rest of the scene and back into the camera
\end{itemize}

\begin{figure}[]
    \centering
    \def\svgwidth{0.6\textwidth}
    \input{images/figures/03-simple_scenario.pdf_tex}
    \caption{An illustration of the scenario modelled by the simple rendering function. The projector is pointed directly at the wall and coincides with the camera which allows for substantial simplification of the rendering process.}
    \label{fig:methods_simple_scenario}
\end{figure}

This scenario (see fig. \ref{fig:methods_simple_scenario} for an illustration) is not too unrealistic -- it is a reasonable approximation of projecting onto a textured wall in a living room where the camera is right next to the projector and thus the projector-camera correspondence is straightforward to recover (as mentioned in section \ref{section:background-projection_mapping-procams-radiometric_calibration}, inter-reflection is minimal in this case and thus a 1:1 correspondence between projector and camera pixels can be assumed). See figs. \ref{fig:intro_grossberg} and \ref{fig:intro_result_teaser} for examples where such a scenario can be used.

Let us now derive a rendering function that handles this simple scenario directly from the rendering equation:

\begin{align}
    L(x \rightarrow \mathbf{v}) &= \int_{\Omega(x)} L(x \leftarrow \mathbf{u}) f_r(x, \mathbf{u} \rightarrow \mathbf{v}) \cos \theta d\mathbf{u} + E(x \rightarrow \mathbf{v}) \label{eq:simple_x_from_rendering_eq01} \\
    &= \int_{\Omega(x)} L(x \leftarrow \mathbf{u}) f_r(x, \mathbf{u} \rightarrow \mathbf{v}) \cos \theta d\mathbf{u} \label{eq:simple_x_from_rendering_eq02} \\
    &= \int_{\Omega(x)} L(x \leftarrow \mathbf{u}) \rho \frac{\cos \theta}{\pi} d\mathbf{u} \label{eq:simple_x_from_rendering_eq03} \\
    &= \int_{\Omega(x)} E(x \leftarrow \mathbf{u}) \rho \frac{\cos \theta}{\pi} d\mathbf{u} \label{eq:simple_x_from_rendering_eq04} \\
    &\approx E(x \leftarrow \mathbf{-v}) \rho \frac{\cos \theta}{\pi r^2} \label{eq:simple_x_from_rendering_eq05}
\end{align}

Eq. \ref{eq:simple_x_from_rendering_eq01} is the rendering equation. We move to eq. \ref{eq:simple_x_from_rendering_eq02} by observing that our background is not an emitter in itself. Eq. \ref{eq:simple_x_from_rendering_eq03} is obtained by assigning the diffuse BRDF function \(\rho / \pi\) to the background surface. \(\rho\) is surface albedo which corresponds to the value of the background image at \(x\). \(\pi\) is a scaling constant needed to preserve energy in the scene. Eq. \ref{eq:simple_x_from_rendering_eq04} comes from our assumption that the only radiance that arrives at point \(x\) on the background comes from the projector. Finally, the integral can be approximated (eq. \ref{eq:simple_x_from_rendering_eq05}) by restricting the area we integrate over to the solid angle subtended by a projector pixel, assuming the integrand is constant over that area and setting \(u = v\). The area is inversely proportional to the square of the distance between the projector pixel and \(x\). Finally, we observe that \(E(x \leftarrow \mathbf{-v}) = p\) and \(\rho\) is the background image.

This leads us to the following straightforward and differentiable rendering function \(x\):

\begin{equation}
    \label{eq:rendering_function-simple}
    x(p, b) = c \cdot p \cdot b = y
\end{equation}

where \(p\) and \(y\) are again the projector and camera image, respectively, \(b\) is the background -- an image (texture) representing the surface of the wall -- and \(c\) is a constant that roughly accounts for \(\frac{\cos \theta}{\pi r^2}\). Here is where we perform the last simplification. \(r\) and \(\theta\) vary over the background and depend on how far the projector is from the background. However, we are only interested in simulating situations where the projector cannot be bright enough to compensate the projection with conventional methods but where it is still capable of compensating with our method. We thus set \(r\) ourselves to adjust the distance and the brightness of the projector according to our needs as part of a single constant \(c\). The variation of \(\cos \theta\) can be accounted for after the fact by darkening \(p\) towards the center. This means that we also have to assume that the minimum brightness of our projector is not limited by the scenario.

To summarize, the simple rendering function \(x\) can be very easily integrated into Gatys' synthesis pipeline and represents negligible computational overhead. This allows for rapid experimentation that yields reference results for a reasonable subset of projection scenarios. However, a more general solution for projection mapping on arbitrary 3D scenes is also needed. We cover it in the next section.

\subsection{General Scenario}
\label{section:methods-rendering_function-general}

A possible candidate for a general rendering function that computes camera images from projector image for arbitrary 3D scenes is simply a renderer such as PBRT (\citet{PBRT3e}) or Mitsuba (\citet{Mitsuba}). These renderers are programs that accept a scene configuration file as input and output an image of the scene as seen from the point of view of a virtual camera. For example, fig. \ref{fig:background_linear_lt} was created using Mitsuba. Renderers often implement a large variety of materials, camera effect, rendering equation solvers and other features that make them incredibly versatile. And indeed, they match the signature of our rendering function well:

\begin{equation}
    \label{eq:rendering_function-renderer}
    x(p, \textit{config}) = y
\end{equation}

where \(x\) is the renderer and \textit{config} represents the configuration file which also describes how the projector image \(p\) is used in the scene. However, renderers come with two main caveats:

\begin{itemize}
    \item They are generally not differentiable
    \item They require a complete description of scene geometry and materials which is rarely available in a projection mapping application
\end{itemize}

Both points are nowadays being addressed by differentiable renderers such as Mitsuba 2 (\citet{Mitsuba2}), but these systems are general-purpose and therefore too computationally heavy to be used in a texture synthesis pipeline. Instead, we use the fact that light transport in an arbitrary scene is linear and model our general rendering function \(x\) with a light transport matrix that was described in section \ref{section:background-projection_mapping-procams-inverse_lt}:

\begin{equation}
    \label{eq:rendering_function-lt_matrix}
    x(p, A) = c \cdot Ax = y
\end{equation}

where \(A\) is the light transport matrix (see \ref{eq:lt_matrix}) and \(p\) is supposed to be a vector representing the light sources in the scene. But this is exactly our projector image where each pixel corresponds to a light source that we can control separately. \(c\) is the same multiplier as in eq. \ref{eq:rendering_function-simple} whose purpose is to tune the overall brightness of the projector.

Multiplication by large matrices is differentiable, very fast on modern GPUs and the matrix size (and thus the computation time) depends only on the resolution of the projector and camera images, not on scene complexity. This makes them easy to use in a texture synthesis pipeline. However, the process of acquiring them is not entirely straightforward. We focus on it in the next section.

\subsubsection{Capturing the Light Transport Matrix}
\label{section:methods-rendering_function-general-lt_capture}

If a scene contains two lamps as light sources, then capturing a light transport matrix with respect to those lamps is nothing more than taking two photographs of the scene -- one for each lamp switched on separately. Our scene contains as many lamps as there are pixels in our projector image \(p\) (we can ignore other light sources because those are fixed). Taking a photograph for each of the projector pixels being switched on separately is practically impossible because of the signal-to-noise ratio of cameras -- no camera is able to capture such tiny signal as that coming from a single projector pixel.

If the goal is to capture an LT matrix of a real scene using a camera, there are other ways of doing it. Capturing one projector pixel at a time corresponds to capturing the canonical basis of the matrix (see fig. \ref{fig:background_lt_capture} for an illustration). As we saw in section \ref{section:background-projection_mapping-procams-inverse_lt}, different bases that comprise brighter pattern can be used as well. The fact that the matrix is often very sparse can also be used.

However, our projector-camera system is simulated fully in software which means that our scenes are virtual and we know precisely which geometry and materials they have. We can thus capture the canonical basis directly using a renderer whose floating-point arithmetic can handle tiny signals. We use Mitsuba to render camera images of our scene illuminated by each projector pixel separately and then assemble the images into the LT matrix \(A\). Note that the LT matrix capture is the main part of our pipeline that we simulate. The rest of the pipeline could work just as well with a different LT matrix \(A\) that would be captured from a real scene by a physical camera. See fig. {\color{red} TODO} for an illustration.

We now describe the capture process in more detail, focusing on how to configure Mitsuba for our purposes and how to work with the LT matrix. Specifically, we look at three topics:

\begin{itemize}
    \item How to implement a projector in Mitsuba?
    \item How to solve the rendering equation efficiently with only a single active projector pixel?
    \item How large is the light transport matrix and what are the implications?
\end{itemize}

\textbf{How to implement a projector in Mitsuba?} The Mitsuba renderer has very rudimentary projector functionality in the form of a spotlight with a projective texture (see fig. \ref{fig:methods_projector_features_spotlight}). This is not enough for our purposes. On the other hand, it is important to set a limit on the number of projector features that we need because some features like realistic optics with multiple thick lenses are difficult to implement while not adding much value to our experiments. Our goal is to have reasonably good-looking projections where the basic limitations we wish to overcome are present: limited gamut and depth of field. We have thus decided to add the following two features to Mitsuba's projector implementation:

\begin{itemize}
    \item Rectangular projection frustum (see fig. \ref{fig:methods_projector_features_frustum})
    \item Thin lens optics (see fig. \ref{fig:methods_projector_features_thin_lens})
\end{itemize}

Rectangular projection frustum is necessary because we want to project rectagular images. The thin lens model (described in section \ref{section:background-projection_mapping-projectors-limitations}) allows us to simulate the depth of field effect with sufficient accuracy. We focus on the upper limit of projector gamut by simply restricting the values of projector image \(p\) to \([0, 1]\) and a suitable choice of multiplier \(c\) (see eq. \ref{eq:rendering_function-lt_matrix}).

\begin{figure}[]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/03-projector_features-spotlight.jpg}
        \caption{}
        \label{fig:methods_projector_features_spotlight}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/03-projector_features-frustum.jpg}
        \caption{}
        \label{fig:methods_projector_features_frustum}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/03-projector_features-thin_lens.jpg}
        \caption{}
        \label{fig:methods_projector_features_thin_lens}
    \end{subfigure}
    \caption{Projector features that we have implemented in Mitsuba to support the needs of our projection mapping pipeline. Image \ref{fig:methods_projector_features_spotlight} shows  a spotlight with a projective texture that Mitsuba already supported but was not sufficient for us. Image \ref{fig:methods_projector_features_frustum} shows a rectangular projection frustum that we have added to support rectangular projections. Image \ref{fig:methods_projector_features_thin_lens} shows our final projector implementation with a thin lens optics model that enables the depth of field effect}
    \label{fig:methods_projector_features}
\end{figure}

\textbf{How to solve the rendering equation efficiently with only a single active projector pixel?} Projecting images with our custom projector implementation yields good results (see fig. \ref{fig:methods_projector_features_thin_lens}). However, in order to capture the light transport matrix, we need to project images with only a single active pixel. And this turns out to be problematic due to the way the rendering equation (\ref{eq:rendering_equation}) is typically solved. A \textit{path tracer} (PT), which is the default Monte Carlo solver in Mitsuba and most other renderers, samples random paths from camera, computes radiance travelling along them in the opposite direction and builds an estimate of the final image. In our case, this means that a random walk from the camera needs to sample a single pixel on the projector to yield a light-carrying path. The probability of this happening is extremely low and most paths thus do not contribute anything to the estimate. This results in poor convergence and extremely long rendering time. See fig. \ref{fig:methods_sampling_path} for an example.

However, while some path sampling techniques build path starting from the camera, others start from the emitter. When emitters are very small, like points lights or projectors with only a single white pixel, it is important to start building the path from the light. We therefore use the bi-directional path tracing (BDPT) algorithm which combines paths that start both from the camera and from emitters and which is already implemented in Mitsuba. More on BDPT can be found in \citet{Veach1997}. See fig. \ref{fig:methods_sampling_bdpt} for an example.

Another issue is with sampling a point on the projector where a path could start from. If uniform sampling is used, the only active pixel would be sampled in 1 out of \(m \cdot n\) samples on average where \(m\) and \(n\) are the dimensions of \(p\). This would again yield many paths with zero contribution. To fix this, we have implemented a common variance reduction technique technique used in Monte Carlo integration called \textit{importance sampling}. Importance sampling chooses samples with larger contribution with higher probability. In our case, we base the probability of choosing a projector pixel on the relative contribution of the pixel to the brightness of the whole projector image. This means that when capturing the canonical basis of an LT matrix, the only active pixel is sampled with probability 1 and other pixels are not sampled at all. See fig. \ref{fig:methods_sampling_bdpt_importance} for an example.

\begin{figure}[]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/03-sampling_path.jpg}
        \caption{}
        \label{fig:methods_sampling_path}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/03-sampling_bdpt.jpg}
        \caption{}
        \label{fig:methods_sampling_bdpt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/03-sampling_bdpt_importance.jpg}
        \caption{}
        \label{fig:methods_sampling_bdpt_importance}
    \end{subfigure}
    \caption{A comparison of various sampling techniques to solve the rendering equation. The scene is being projected onto by a \(640 \times 480\) image with only a small \(20 \times 20\) patch of active pixels. All images were rendered using 16 path samples per pixel. Image \ref{fig:methods_sampling_path} shows a basic path tracer which samples random light-carrying paths starting from the camera. Image \ref{fig:methods_sampling_bdpt} is bi-directional path tracing (BDPT), a better technique which combines paths from both camera and light sources. Image \ref{fig:methods_sampling_bdpt_importance} shows BDPT with importance sampling of the projector image which greatly reduces noise. This is the method we use in our pipeline}
    \label{fig:methods_sampling}
\end{figure}

\textbf{How large is the light transport matrix and what are the implications?} The last practical consideration we need to make is about the size of the light transport matrix. Here is how it is determined:

\begin{equation}
    \label{eq:lt_matrix_size}
    S = p_w \cdot p_h \cdot c_w \cdot c_h \cdot 3 \cdot 4
\end{equation}

where \(S\) is the matrix size in bytes, \(p_w\) and \(p_h\) is the width and height of the projector image, respectively, \(c_w\) and \(c_h\) are the dimensions of the camera image. We then assume three color channels and four bytes to store each intensity value. It is important to store the intensity values as 32-bit floating point numbers to capture the tiny signal coming from individual projector pixels. See an example of how value precision influences the noise level of basis images in fig. \ref{fig:methods_float}.

This means that an LT matrix for projector and camera image of size \(160 \times 160\) is over 7.8 GB large. This is the size we have used in our experiments because it is sufficient to demonstrate the performance of our method and because it fit well into the memory of our GPUs. However, it is clear that this approach to implementing the rendering function is not very scalable. For our purposes, this approach has the advantage of being simple and accurate which is important when evaluating the basic functionality of a new method. For practical use, more data-efficient methods like those mentioned in section \ref{section:background-projection_mapping-procams-inverse_lt} are needed.

\begin{figure}[]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/03-floating_point16.jpg}
        \caption{}
        \label{fig:methods_float16}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/03-floating_point32.jpg}
        \caption{}
        \label{fig:methods_float32}
    \end{subfigure}
    \caption{An illustration of how pixel value precision influences noise level in basis images. The scene is illuminated by a \(640 \times 480\) image where only a single pixel is active. The resulting images are scaled by \(1000000\) for visualization purposes. Image \ref{fig:methods_float16} is stored in 16-bit floating point, while image \ref{fig:methods_float32} is stored in 32-bit floating point}
    \label{fig:methods_float}
\end{figure}

This concludes the section on our rendering function \(x\). In the following section we cover the second building block of our projection mapping pipeline, the texture model \(f\).

\section{Texture Model}
\label{section:methods-texture_model}

Our texture model is built on top of the CNN-based texture synthesis of \citet{Gatys2015} (described in section \ref{section:background-texture_synthesis-statistics_based}) with minor improvements introduced by other authors in recent years. Let us first explain why we have chosen this model over other altenatives outlined in section \ref{section:background-texture_synthesis}.

As mentioned in section \ref{section:background-texture_synthesis-patch_based-projection_mapping}, patch-based texture synthesis could theoretically be adapted for projection mapping purposes. However, this approach would result in sub-optimal performance mostly for two reasons:

\begin{itemize}
    \item Assumption of 1:1 correspondence between projector and camera image pixels would be needed
    \item Patch-based methods perform very well for certain textures, but the latest statistics-based methods perform better overall as they are more general ({\color{red} TODO: some examples to illustrate this?})
\end{itemize}

Statistics-based methods have the great advantage of usually consisting of an optimization loop that our rendering function (as described in section \ref{section:methods-rendering_function}) can be integrated into. We can thus easily account for general light transport without any simplifying assumptions.

Out of all statistics-based methods, those that are based on CNNs achieve the best results. We have chosen that of \citet{Gatys2015} as our basis because it is well studied and simple to implement. Other authors have also introduced many techniques to improve it which we wanted to test to see if they also provide improvements in projection mapping.

\citet{Gatys2015} have provided an implementation of their algorithm in the Caffe (\citet{Jia2014}) deep learning framework. Because this framework is not very commonly used anymore, we have ported the algorithm to PyTorch which is currently the most commonly used deep learning framework ({\color{red} TODO: some proof for this and for Caffe being obscure?}). As a result, our method will be easier to work with and expand upon in the future.

While the original Gatys texture synthesis provides very high quality results, it also has several shortcomings that have been identified in recent years ({\color{red} TODO: example images!}). We have thus also implemented various known improvements with the intention of evaluating them in our projection mapping setup.

\subsection{Improving Gatys Texture Synthesis}
\label{section:methods-texture_model-improvements}

We have implemented three different improvements to the original Gatys algorithm. In general, all of them then to tighten the statistical description of a texture (i.e. reduce the size of texture classes as defined by their texture model) and thus better enforce texture structure while maintaining high variance of outputs. We now briefly describe each of the three improvements.

\subsubsection{Activation Shift}
\label{section:methods-texture_model-improvements-activation_shift}

\citet{Novak2016} have introduced various improvements to the algorithm of style transfer ({\color{red} TODO: citation}) which is very similar to texture synthesis. In its core, it optimizes a white noise image to match the statistics of two input images (as opposed to only one in texture synthesis). The goal is to match the \textit{style} of one image and \textit{content} of the other. The style of an image is defined in exactly the same way as the description of a texture in Gatys synthesis. This is why the improvements of \citet{Novak2016} that concern image style are directly applicable to texture synthesis as well.

One of their improvements is called activation shift. The activations of VGG-19 that are used to compute Gram matrices and thus the description of a texture image are non-negative with mean 1. Also, the resulting Gram matrices tend to be very sparse with a lot of zeroes. And since each Gram matrix entry comes from the multiplication of entries from different activations in a VGG-19 layer, \citet{Novak2016} argue that these zeroes are a source of ambiguity because they could have many causes:

\begin{itemize}
    \item Both features that the activations represent are missing from the image (\(0 \cdot 0\))
    \item Only one of the features is present (\(0 \cdot a\))
    \item Both of them are present but never appear together (\(0 \cdot a\))
\end{itemize}

To remove this ambiguity, \citet{Novak2016} suggest to offset the activations by \(-1\) before computing the Gram matrices. This is extremely simple to implement and brings clear improvements to synthesis quality.

{\color{red} TODO: example of the improvement in synthesis}

{\color{red} TODO: use math notation to make it easier to follow?}

\subsubsection{Correlation Chain}
\label{section:methods-texture_model-improvements-correlation_chain}

The next improvement also comes from \citet{Novak2016}. It modifies the way Gram matrices are calculated. In the original algorithm, only activations from the same layer are correlated with each other to form a Gram matrix. \citet{Novak2016} suggest to correlate activations from different layers together, specifically neighbouring layers. For example, instead of having Gram matrices that correspond to correlations of layers 1:1, 2:2, 3:3 and 4:4, using a correlation chain means to build Gram matrices 1:2, 2:3, 3:4 instead, possibly to use them alongside the original ones to further lower the ambiguity of the resulting texture description.

There is, however, one caveat here. Activations of neighbouring layers might have different dimensions. To be able to correlate them together, \citet{Novak2016} therefore upsample the smaller matrix of activations to match the dimensions of the larger one ({\color{red} TODO: which filter? box?}).

{\color{red} TODO: example of the improvement in synthesis}

{\color{red} TODO: use math notation to make it easier to follow?}

\subsubsection{Gaussian Pyramid}
\label{section:methods-texture_model-improvements-gaussian_pyramid}

\citet{Snelgrove2017} has noticed that Gatys' synthesis algorithm struggles to reproduce texture features of certain sizes, particularly when a variety of them is present in a single texture ({\color{red} TODO: example images!}). He attributes this to the limited receptive field of the underlying VGG-19 that was trained on images of size \(224 \times 224\). Indeed, the original algorithm does not work very well when synthesizing images that are much larger than \(224 \times 224\) and it is also true that larger images tend to have larger features.

To fix this, \citet{Snelgrove2017} proposes to not compute the Gram matrices on images, but rather on Gaussian pyramids of these images. This means that instead of feeding an image to the VGG-19 network, we downsample the image to multiple smaller scales (using proper pre-filtering, for example Lanczos ({\color{red} TODO: cite?})) and feed each of these to the network, computing Gram matrices for each scale of the original image. This tightens the texture description yet again, this time ensuring that features of multiple sizes are correctly matched.

{\color{red} TODO: example of the improvement in synthesis}

{\color{red} TODO: use math notation to make it easier to follow?}