\chapter{Methods}
\label{chapter:methods}

We now have all the building blocks needed to construct our method that implements projection mapping of textures by forcing the camera image to be a realization of the same texture as the desired appearance, instead of matching them pixel by pixel which is the usual approach (see section \ref{section:intro-key_idea} for more details on our key idea).

We will first present an overview of our projection mapping pipeline and then focus on its main components.

\section{Pipeline Overview}
\label{section:methods-pipeline_overview}

\begin{figure}[]
    \centering
    \def\svgwidth{\textwidth}
    \input{images/figures/03-pipeline.pdf_tex}
    \caption{An overview of our pipeline with \(y\) as input and \(p\) that minimizes eq. \ref{eq:projection_mapping-statistics} as output. The blue part of the diagram is essentially a texture synthesis pipeline by \citet{Gatys2015}. The green part (the rendering function) is our extension that enables projection mapping. Instead of computing a set of statistics \(f\) on the synthesized image \(p\) directly as \citet{Gatys2015} does (blue arrow), we first project \(p\) to obtain \(x(p)\) and only then compute \(f\). The red projector symbol shows how \(p\) is projected onto a glass ball in our scene and the white scissors symbol shows how the camera image is cropped because we only want to match a portion of the scene. The whole process is an optimization loop with \(p\) initialized to white noise and progressively refined until \(x(p)\) is a realization of the same texture as \(y\) is. Texture source: \citet{Pixar128}}
    \label{fig:methods_pipeline}
\end{figure}

Projection mapping is typically done with a projector-camera system (see fig. \ref{fig:intro_procam} for an example). However, we have decided to implements our system entirely as a software pipeline with simulated projector and camera for the following reasons:

\begin{itemize}
    \item A software implementation serves as necessary reference for future work. If something does not work in the controlled environment of a simulator, it will not work in the real world either
    \item Hardware systems come with challenges of its own, like signal-to-noise ratio, dynamic environment and time constraints. We want to focus on solving one challenge at a time, starting with projection quality
    \item Software pipelines are easier to modify and allow for rapid prototyping and experimentation
\end{itemize}

% 1) MAKE THE BIG FIGURE
% 2) DESCRIBE IT
% 3) PROVIDE A FEW JUSTIFICATIONS OF THE DESIGN (why not quilting and so on...)

The goal of our pipeline is to minimize the expression in eq. \ref{eq:projection_mapping-statistics}. The main input is an image \(y\) which represents the desired appearance for our scene. Other inputs are the scene and projector which determine a rendering function \(x\) which creates a camera image \(x(p)\) out of a projector image \(p\). A projector image \(p\) which minimizes eq. \ref{eq:projection_mapping-statistics} is the output of the pipeline. See fig. \ref{fig:methods_pipeline} an illustration of how the pipeline works.

We build on top of a pipeline for texture synthesis used in \citet{Gatys2015} and extend it by the rendering function \(x\). The original synthesis pipeline is differentiable and relies on the gradient-based L-BFGS optimizer to synthesize a texture. Therefore, it is key that our \(x\) differentiable as well. This allows us to rely on L-BFGS to minimize eq. \ref{eq:projection_mapping-statistics}.

In section \ref{section:background-texture_synthesis}, we have reviewed several texture synthesis methods that could potentially be used in our method. We have decided to use \citet{Gatys2015} over a patch-based method such as \citet{Efros2001} for the following reasons:

\begin{itemize}
    \item It achieves good results for a large variety of textures
    \item It defines a texture model which can be (and has been, as we will discuss later) improved to achieve better results while keeping the rest of the pipeline fixed
    \item It is very easy to adapt for projection mapping and does not impose any restrictions on the rendering function. In fact, a gradient-free version of the pipeline could theoretically be run on a physical projector-camera system
    \item Using an optimizer to gradually improve the result allows us to obtain a reasonable result quickly and improve it if more time is available
\end{itemize}

We will now describe the two main components of our pipeline -- the rendering function \(x\) and the texture model \(f\) -- in more detail.

\section{Rendering Function}
\label{section:methods-rendering_function}

The rendering function in our pipeline should have the following form:

\begin{equation}
    \label{eq:rendering_function}
    f(x, \phi) = y  
\end{equation}

where \(x\) is the projector image, \(\phi\) are scene parameters, such as geometry, materials and projector and camera position and orientation. \(y\) is then the camera image whose statistics would be compared with those of the input texture image. It also need to be differentiable, so that gradients can flow through it in our optimization loop.

\(f\) therefore represents the act of projecting an image onto a scene and then capturing that projection. This is a fairly complex task, especially if the scene can have arbitrary geometry, materials and external illumination. Luckily, in certain special scenarios, \(f\) can be very simple to formulate and fast to compute. We take advantage of this and use two different versions of \(f\) in our pipeline, one simple to verify basic functionality and obtain reference results, and one complex to support arbitrary scenes.

\subsection{Simple Scenario}
\label{section:methods-rendering_function-simple}

Here is a scenario in which the light transport between the projector and the camera is easy to model:

\begin{itemize}
    \item The projector is pointed directly at a flat wall, with a right angle between its optical axis and the surface of the wall
    \item The surface of the wall is completely diffuse, meaning it looks matte and its appearance is not view-dependent
    \item The camera coincides with the projector
    \item We ignore light that is reflected from the wall into the rest of the scene and back into the camera
\end{itemize}

The correct rendering function \(f\) in this case is

\begin{equation}
    \label{eq:rendering_function-simple}
    f(x, b) = c \cdot x \cdot b = y
\end{equation}

where \(x\) and \(y\) are again the projector and camera image, respectively, \(b\) is the background -- an image (texture) representing the surface of the wall -- and \(c\) is a factor that attenuates the light travelling between the projector and the camera ({\color{red} TODO: is this \(1/r^2\)?}). In our case, we fix \(c\) to a hand-picked value (either 1 or 5 in our experiments) because it acts against the power of the projector which we also need to model somehow. Given that values of \(x\) and \(y\) lie in \([0, 1]\), choosing a larger \(c\) will make the projector brighter. We have not calibrated this value to match a real projector because we are interested in cases where the ideal \(x\) is just outside the color gamut of the projector. We therefore always choose a \(c\) so that the scene is just a little too difficult to project onto.

This function is indeed very simple, easily differentiable and covers a scenario which is not too unrealistic -- it is a reasonable approximation of projecting onto a textured wall in a living room where the camera is right next to the projector and thus the projector-camera correspondence is straightforward to recover (as mentioned in section \ref{section:background-projection_mapping-procams-radiometric_calibration}, inter-reflection is minimal in this case and thus a 1:1 correspondence between projector and camera pixels can be assumed).

\subsection{Complex Scenario}
\label{section:methods-rendering_function-complex}

What if we want to project on an arbitrary 3D scene? One candidate for our rendering function \(f\) in this case is simply a renderer, such as PBRT (\citet{PBRT3e}) or Mitsuba (\citet{Mitsuba}), which implements a projector and all other effects and scene materials that our scene contains:

\begin{equation}
    \label{eq:rendering_function-renderer}
    f(x, \text{config}) = \text{renderer}(x) = y
\end{equation}

where \(x\) and \(y\) are again the projector and camera image, respectively, \textit{config} is the configuration file that describes the scene to the renderer and \textit{renderer} is an invocation of the renderer program with that input producing \(y\) as output. The problem with this setup is that such an \(f\) would not be differentiable with respect to \(x\) ({\color{red} TODO: Mitsuba 2?}). Also, a full rendering program is very computationally heavy and the time it would take to arrive at \(y\) would depend on the complexity of the scene and the resolution of the camera image, reaching up to several minutes or hours for challenging scenes. Lastly, we would need to know the exact geometry and materials of our scene in order to set up the renderer ({\color{red} TODO: could Mitsuba 2 help here as well?})

Instead, we use the fact that light transport is linear and model our \(f\) with a light transport matrix, as mentioned in section \ref{section:background-projection_mapping-procams-inverse_lt}:

\begin{equation}
    \label{eq:rendering_function-lt_matrix}
    f(x, A) = Ax = y
\end{equation}

where \(A\) is the light transport matrix (see \ref{eq:lt_matrix}). \(x\) is supposed to be a vector representing the light sources in the scene. But this is exactly our projector image where each pixel corresponds to a light source that we can control separately.

Multiplication by large matrices is differentiable, very fast on modern GPUs and the matrix size (and thus the computation time) depends only on the resolution of the projector and camera images, not on scene complexity. This time, we do not need to invert the matrix as in section \ref{section:background-projection_mapping-procams-inverse_lt}, but we still need to capture it.

\subsubsection{Capturing the Light Transport Matrix}
\label{section:methods-rendering_function-complex-lt_capture}

If a scene contains two lamps, then capturing a light transport matrix with respect to those lamps is nothing more than taking two photographs of the scene -- one for each lamp switched on separately. Our scene contains as many lamps as there are pixels in our projector image (we can ignore other light sources because we keep those in the same state throughout the process). Taking a photograph for each of the projector pixels being switched on would be two things:

\begin{itemize}
    \item Capturing the canonical basis of the light transport matrix
    \item Impossible
\end{itemize}

It is impossible to do this in the real world because of the signal-to-noise ratio of cameras -- no camera is able to capture such faint signal as that coming from a single projector pixel. But as we saw in section \ref{section:background-projection_mapping-procams-inverse_lt}, there are other ways of capturing the light transport matrix, using a different basis to capture (one with more striking brightness differences in the camera images) and taking advantage of the fact that the light transport matrix is ofter very sparse.

However, we are making a simulator and thus can afford to capture the light transport matrix via the canonical basis. We use the Mitsuba renderer (\citet{Mitsuba}) to render images camera images of our scene illuminated by each projector pixel turn and then assemble the images into the light transport matrix \(A\). There are, however, a few issues that need to be solved first.

{\color{red} TODO: figure?}

\textbf{How to simulate a projector in Mitsuba?} The Mitsuba renderer has very rudimentary projector functionality in the form of a spotlight with a projective texture ({\color{red} TODO: figure}). This is not enough for our purposes. On the other hand, it is important to set a limit on the number of projector features that we need because implementing a realistic projector is very challenging. It may involve things like realistic thick lens optics, a color model without truly deep blacks as described in section \ref{section:background-projection_mapping-projectors-DLP} and so on. We have decided to add the following two features to Mitsuba's projector implementation:

\begin{itemize}
    \item Rectangular projection frustum ({\color{red} TODO: figure})
    \item Thin lens model ({\color{red} TODO: figure})
\end{itemize}

Rectangular projection frustum is necessary because we want to project rectagular images. Thin lens model is a simplified lens model that allows for depth of field effects. These are also necessary because no projector image is sharp at all depths at the same time and image defocus is a common effect that we wish to compensate. No other features are needed because already with these two the projections look realistic and exhibit the main hardware limitations we wish to overcome -- brightness and blur.

{\color{red} TODO: How \textit{exactly} are these implemented? Since you're not sure yourself, it would be nice to add some nice figures with descriptions into the appendix (if it's not suitable here).}

\textbf{How to render efficiently with only a single projector pixel turned on?} If we want to project images with our custom projector implementation described above, we get good results. However, in order to capture the light transport matrix, we need to project images with only a single pixel turned on. And this turns out to be problematic due to the way the rendering equation (\ref{eq:rendering_equation}) is solved in renderers like Mitsuba. These methods sample random light paths, compute light travelling along those and build the full estimate via a Monte Carlo approach. This means that also our projection image needs to be sampled, so that the sampled paths cover the light provided by each of the pixels. But sampling paths is costly and if the projector image with a single white pixel was sampled uniformly, most paths would have zero contribution. This would result in noisy renderings and long rendering times ({\color{red} TODO: figure}). To fix this, we implemented a common technique called importance sampling ({\color{red} TODO: cite?}) which samples paths with larger contribution with higher probability. As a matter of fact, black pixels of the projector image should never be sampled at all.

Moreover, some path sampling techniques build path starting from the camera, while others start from the emitter. When emitters are very small, like points lights or projectors with only a songle white pixel, it is important to start building the path from the light. Otherwise, the probability of sampling a path with non-zero contribution would again be too small. To do that, we use the bi-directional path tracing algorithm which is implemented in Mitsuba. {\color{red} TODO: figure with the improvement}

{\color{red} TODO: Time for another appendix section on how the rendering equation is solved via path tracing?}

\textbf{How large is the light transport matrix and what are the implications?} The last practical consideration we need to make is about the size of the light transport matrix. Here is how it is determined:

\begin{equation}
    \label{eq:lt_matrix_size}
    S = p_w \cdot p_h \cdot c_w \cdot c_h \cdot 3 \cdot 4
\end{equation}

where \(S\) is the matrix size in bytes, \(p_w\) and \(p_h\) is the width and height of the projector image, respectively, \(c_w\) and \(c_h\) are the dimensions of the camera image. We then assume three color channels and four bytes to store each intensity value. It is important to store the intensity values as 32-bit floating point numbers, otherwise the basis images forming the matrix would be too noisy ({\color{red} TODO: I had an example somewhere, right?}).

This means that a light transport matrix for projector and camera image of size \(160 \times 160\) is over 7.8 GB large. While this is the size we have used in our experiments because it is sufficient to demonstrate certain effects and because it fit well into the memory of our GPUs, it is clear that this approach to implementing the rendering function is not very scalable. For our purposes, this approach has the advantage of being simple and accurate which is important when evaluating the basic functionality of a new method. For practical use, a more data-efficient method is needed.

\section{Texture Model}
\label{section:methods-texture_model}

Our texture model is built on top of the CNN-based texture synthesis of \citet{Gatys2015} (described in section \ref{section:background-texture_synthesis-statistics_based}) with minor improvements introduced by other authors in recent years. Let us first explain why we have chosen this model over other altenatives outlined in section \ref{section:background-texture_synthesis}.

As mentioned in section \ref{section:background-texture_synthesis-patch_based-projection_mapping}, patch-based texture synthesis could theoretically be adapted for projection mapping purposes. However, this approach would result in sub-optimal performance mostly for two reasons:

\begin{itemize}
    \item Assumption of 1:1 correspondence between projector and camera image pixels would be needed
    \item Patch-based methods perform very well for certain textures, but the latest statistics-based methods perform better overall as they are more general ({\color{red} TODO: some examples to illustrate this?})
\end{itemize}

Statistics-based methods have the great advantage of usually consisting of an optimization loop that our rendering function (as described in section \ref{section:methods-rendering_function}) can be integrated into. We can thus easily account for general light transport without any simplifying assumptions.

Out of all statistics-based methods, those that are based on CNNs achieve the best results. We have chosen that of \citet{Gatys2015} as our basis because it is well studied and simple to implement. Other authors have also introduced many techniques to improve it which we wanted to test to see if they also provide improvements in projection mapping.

\citet{Gatys2015} have provided an implementation of their algorithm in the Caffe (\citet{Jia2014}) deep learning framework. Because this framework is not very commonly used anymore, we have ported the algorithm to PyTorch which is currently the most commonly used deep learning framework ({\color{red} TODO: some proof for this and for Caffe being obscure?}). As a result, our method will be easier to work with and expand upon in the future.

While the original Gatys texture synthesis provides very high quality results, it also has several shortcomings that have been identified in recent years ({\color{red} TODO: example images!}). We have thus also implemented various known improvements with the intention of evaluating them in our projection mapping setup.

\subsection{Improving Gatys Texture Synthesis}
\label{section:methods-texture_model-improvements}

We have implemented three different improvements to the original Gatys algorithm. In general, all of them then to tighten the statistical description of a texture (i.e. reduce the size of texture classes as defined by their texture model) and thus better enforce texture structure while maintaining high variance of outputs. We now briefly describe each of the three improvements.

\subsubsection{Activation Shift}
\label{section:methods-texture_model-improvements-activation_shift}

\citet{Novak2016} have introduced various improvements to the algorithm of style transfer ({\color{red} TODO: citation}) which is very similar to texture synthesis. In its core, it optimizes a white noise image to match the statistics of two input images (as opposed to only one in texture synthesis). The goal is to match the \textit{style} of one image and \textit{content} of the other. The style of an image is defined in exactly the same way as the description of a texture in Gatys synthesis. This is why the improvements of \citet{Novak2016} that concern image style are directly applicable to texture synthesis as well.

One of their improvements is called activation shift. The activations of VGG-19 that are used to compute Gram matrices and thus the description of a texture image are non-negative with mean 1. Also, the resulting Gram matrices tend to be very sparse with a lot of zeroes. And since each Gram matrix entry comes from the multiplication of entries from different activations in a VGG-19 layer, \citet{Novak2016} argue that these zeroes are a source of ambiguity because they could have many causes:

\begin{itemize}
    \item Both features that the activations represent are missing from the image (\(0 \cdot 0\))
    \item Only one of the features is present (\(0 \cdot a\))
    \item Both of them are present but never appear together (\(0 \cdot a\))
\end{itemize}

To remove this ambiguity, \citet{Novak2016} suggest to offset the activations by \(-1\) before computing the Gram matrices. This is extremely simple to implement and brings clear improvements to synthesis quality.

{\color{red} TODO: example of the improvement in synthesis}

{\color{red} TODO: use math notation to make it easier to follow?}

\subsubsection{Correlation Chain}
\label{section:methods-texture_model-improvements-correlation_chain}

The next improvement also comes from \citet{Novak2016}. It modifies the way Gram matrices are calculated. In the original algorithm, only activations from the same layer are correlated with each other to form a Gram matrix. \citet{Novak2016} suggest to correlate activations from different layers together, specifically neighbouring layers. For example, instead of having Gram matrices that correspond to correlations of layers 1:1, 2:2, 3:3 and 4:4, using a correlation chain means to build Gram matrices 1:2, 2:3, 3:4 instead, possibly to use them alongside the original ones to further lower the ambiguity of the resulting texture description.

There is, however, one caveat here. Activations of neighbouring layers might have different dimensions. To be able to correlate them together, \citet{Novak2016} therefore upsample the smaller matrix of activations to match the dimensions of the larger one ({\color{red} TODO: which filter? box?}).

{\color{red} TODO: example of the improvement in synthesis}

{\color{red} TODO: use math notation to make it easier to follow?}

\subsubsection{Gaussian Pyramid}
\label{section:methods-texture_model-improvements-gaussian_pyramid}

\citet{Snelgrove2017} has noticed that Gatys' synthesis algorithm struggles to reproduce texture features of certain sizes, particularly when a variety of them is present in a single texture ({\color{red} TODO: example images!}). He attributes this to the limited receptive field of the underlying VGG-19 that was trained on images of size \(224 \times 224\). Indeed, the original algorithm does not work very well when synthesizing images that are much larger than \(224 \times 224\) and it is also true that larger images tend to have larger features.

To fix this, \citet{Snelgrove2017} proposes to not compute the Gram matrices on images, but rather on Gaussian pyramids of these images. This means that instead of feeding an image to the VGG-19 network, we downsample the image to multiple smaller scales (using proper pre-filtering, for example Lanczos ({\color{red} TODO: cite?})) and feed each of these to the network, computing Gram matrices for each scale of the original image. This tightens the texture description yet again, this time ensuring that features of multiple sizes are correctly matched.

{\color{red} TODO: example of the improvement in synthesis}

{\color{red} TODO: use math notation to make it easier to follow?}